\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{nips_2017}
\usepackage{natbib}
\setcitestyle{numbers,open={[},close={]}}

\title{Deep Multimodal Generative Models for Weakly-Supervised Learning}

\author{
    Mike Wu, Noah Goodman \\
    Department of Computer Science \\
    Stanford University \\
    \texttt{{wumike, ngoodman}@cs.stanford.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
TODO
\end{abstract}

\section{Introduction}
The problem of learning from multiple modalities is especially important since the majority of modern information is represented through multiple channels. For example, images in the web are often embedded around text. And while images can be described by pixels, we expect related modalities like text to also hold relevant features. People often interact with this multimodal information in a \textit{bi-directional} way. For example, one can imagine what a black cat looks like but also prescribe this description to a photo. Additionally, available multimodal information is often \textit{sparse}. It is sparse in the sense there exist many large corpuses of images or text alone but few datasets exist (and are expensive to construct) that contain multiple modalities together. Ideally, we would like the generative models we build to extract a joint representation that captures high-level concepts across modalities in a way that is bi-directional and weakly-supervised. 

In this work, we propose a multimodal variational autoencoder (MMVAE). In our graphical model, we have two modalities $x_{1}$ and $x_{2}$ that are each conditioned on a latent variable $z$, which models the joint distribution, $p(x_{1}, x_{2})$. Such a modal is bi-directional since we can reconstruct both modalities from either $x_{1}$ or $x_{2}$. We will also show that we can train the MMVAE with sparse $(x_{1}, x_{2})$ pairs, and heavily utilize on a larger set of independent $(x_{1})$ and $(x_{2})$ examples. The most significant feature of MMVAEs is that it does not create a separate multimodal encoder $q(z | x_{1}, x_{2})$; instead it shares parameters with the encoders from each modality, $q(z | x_{1})$, $q(z | x_{2})$. This has the benefit of producing a simpler model while ensuring that high-dimensional missing modalities do not cause our generated samples to collapse.

\subsection{Related Work}
In deep learning, multimodal learning is often done using separate branches in the neural net for each modality, and a common top hidden layer. For instance, \citet{ngiam2011multimodal} trained deep autoencoders on audio and video input and found that the bimodal representations were better than any single equivalent. More recently, VAEs (\cite{kingma2013auto, kingma2014semi}) have been used to train models in high-dimensional multimodal settings. For example, conditional VAEs (\cite{sohn2015learning}) maximize a conditional log-likleihood by variational methods where often one modality is conditioned on another i.e. handwriting digits and labels, faces and attributes, or captioning. Notably, conditional VAEs (CVAE) are not bi-directional. 

\citet{pandey2017variational} proposed the conditional multimodal autoencoder (CMMA), which also maximizes a conditional log-likelihood. CMMA explicitly connects the conditional variable the latent variable, where CVAE does not. Moreover, it also constrains the latent representation to be close to the joint representation across modalities. However, CMMA is still not bi-directional.

Finally, \citet{suzuki2016joint} proposed a joint multimodal VAE (JMVAE). This is similar to our work in that it is bi-directional, handles missing modalities, and models the joint distribution. Our proposed model (MMVAE) and JMVAE share the same graphical notation, but differ in our approach. Crucially, JMVAE trains separate encoders for each modality and an additional one for the multimodal case. It then includes a KL divergence term \textit{per modality} in the loss. In constract, MMVAE does not have an additional encoder for the multimodal case and does not have additional KL terms. It does so by sharing encoder parameters to learn the guide distributions: $q_{\phi_{x_{1}}}(z|x_{1})$, $q_{\phi_{x_{2}}}(z|x_{2})$, $q_{\phi}(z|x_{1}, x_{2})$

\section{Methods}
This section first briefly goes over VAEs, and product of Gaussians (PoG), then details a new multimodal VAE (MMVAE).

\subsection{Variational Autoencoders}
\subsection{Product of Gaussians}
\subsection{Multimodal Variational Autoencoders}
\subsection{Bidirectional Inference}


\bibliographystyle{abbrvnat}
{\small
\linespread{1}
\bibliography{draft}
}

\end{document}